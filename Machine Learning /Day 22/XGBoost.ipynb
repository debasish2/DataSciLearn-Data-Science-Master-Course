{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bf080a",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "1. XGBoost stands for eXtreme Gradient Boosting.\n",
    "2. XGBoost, a popular machine learning algorithm known for its efficiency and performance in classification and regression tasks.\n",
    "3. XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that has gained immense popularity in recent years due to its exceptional performance on a wide range of problems. It is an implementation of the gradient boosting algorithm, which is a type of ensemble learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e1c14",
   "metadata": {},
   "source": [
    "###  XGBoost with intuition:\n",
    "\n",
    "1. Start with a Weak Learner:\n",
    "\n",
    "- XGBoost starts with a weak learner, which could be a decision tree with a single node. This initial tree simply predicts the average price of all houses in the dataset.\n",
    "\n",
    "2. Calculate Errors\n",
    "\n",
    "- Then, XGBoost calculates the errors between the predicted prices and the actual prices of the houses.\n",
    "\n",
    "3. Build a Tree to Correct Errors\n",
    "\n",
    "-  Now, XGBoost builds a new decision tree to correct these errors. It tries to find patterns in the data that the initial tree missed. This tree is fitted on the errors (residuals) from the previous predictions.\n",
    "\n",
    "4. Update Predictions\n",
    "\n",
    "- The predictions from the new tree are added to the predictions of the previous tree, and the combined predictions are used to update the model's understanding of the data.\n",
    "\n",
    "5. Repeat the Process: \n",
    "\n",
    "- Steps 2-4 are repeated iteratively. Each new tree is built to correct the errors of the combined predictions of all previous trees.\n",
    "\n",
    "6. Final Prediction\n",
    "\n",
    "- Eventually, XGBoost combines the predictions from all the trees to make a final prediction. The final prediction is the sum of the predictions from all the trees.\n",
    "\n",
    "\n",
    "\n",
    "##### The \"gradient\" in XGBoost refers to the gradient of the loss function used in optimization. By iteratively fitting new trees to the gradients of the loss function, XGBoost improves the model's performance with each iteration.\n",
    "\n",
    "##### In summary, XGBoost builds an ensemble of weak learners (decision trees) in a sequential manner, where each new tree corrects the errors of the combined predictions of all previous trees. This iterative process allows XGBoost to create a powerful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0275114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c372c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df80b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30e7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19151a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0a88d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461b11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961a20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37942bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
