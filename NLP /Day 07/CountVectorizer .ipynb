{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df42357",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    "CountVectorizer is a text preprocessing tool provided by the scikit-learn library in Python. It converts a collection of text documents into a matrix of token (word) counts. This is a common first step in preparing text data for machine learning models, especially for tasks like text classification, document classification, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b0225",
   "metadata": {},
   "source": [
    "### How CountVectorizer Works\n",
    "\n",
    "1. Tokenization: The text is split into individual tokens (typically words).\n",
    "2. Vocabulary Building: A vocabulary is built from the tokens across all documents. Each unique token is assigned a unique integer index.\n",
    "3. Document-Term Matrix Creation: For each document, a vector is created where each element corresponds to the count of a particular token (as per the vocabulary) in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a80919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['be' 'can' 'challenges' 'challenging' 'fun' 'is' 'learning' 'machine'\n",
      " 'make']\n",
      "Document-Term Matrix:\n",
      " [[0 0 0 0 1 1 1 1 0]\n",
      " [1 1 0 1 0 0 2 1 0]\n",
      " [0 0 1 0 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"Machine learning is fun.\",\n",
    "    \"Learning machine learning can be challenging.\",\n",
    "    \"Challenges make learning fun.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array (optional, for better readability)\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Vocabulary:\", feature_names)\n",
    "print(\"Document-Term Matrix:\\n\", X_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c40f3",
   "metadata": {},
   "source": [
    "### Output Explanation\n",
    "\n",
    "- Vocabulary: The unique tokens found across all documents\n",
    "['be', 'can', 'challenging', 'challenges', 'fun', 'is', 'learning', 'machine', 'make']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75331946",
   "metadata": {},
   "source": [
    "### Document-Term Matrix: \n",
    "- A matrix where each row represents a document and each column represents a token from the vocabulary. The values are the counts of each token in the respective document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d240c",
   "metadata": {},
   "source": [
    "- [[0 0 0 0 1 1 1 1 0]  # \"Machine learning is fun.\"\n",
    "-  [1 1 1 0 0 0 2 1 0]  # \"Learning machine learning can be challenging.\"\n",
    "-  [0 0 0 1 1 0 1 0 1]] # \"Challenges make learning fun.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a359e2e",
   "metadata": {},
   "source": [
    "### How the Document-Term Matrix is Formed Using CountVectorizer\n",
    "\n",
    "Let's break down how the document-term matrix is formed using the `CountVectorizer`:\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Tokenization**: Split each document into individual tokens (words).\n",
    "2. **Vocabulary Building**: Create a vocabulary of all unique tokens from the corpus.\n",
    "3. **Matrix Construction**: For each document, count the occurrences of each token in the vocabulary and place these counts in the appropriate positions in the matrix.\n",
    "\n",
    "### Given Data\n",
    "Let's start with the documents:\n",
    "\n",
    "1. \"Machine learning is fun.\"\n",
    "2. \"Learning machine learning can be challenging.\"\n",
    "3. \"Challenges make learning fun.\"\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Document 1: `[\"machine\", \"learning\", \"is\", \"fun\"]`\n",
    "   - Document 2: `[\"learning\", \"machine\", \"learning\", \"can\", \"be\", \"challenging\"]`\n",
    "   - Document 3: `[\"challenges\", \"make\", \"learning\", \"fun\"]`\n",
    "\n",
    "2. **Vocabulary Building**\n",
    "   - Create a list of all unique tokens from the documents:\n",
    "     ```\n",
    "     ['be', 'can', 'challenging', 'challenges', 'fun', 'is', 'learning', 'machine', 'make']\n",
    "     ```\n",
    "   - Each word is assigned an index:\n",
    "     ```\n",
    "     'be': 0\n",
    "     'can': 1\n",
    "     'challenging': 2\n",
    "     'challenges': 3\n",
    "     'fun': 4\n",
    "     'is': 5\n",
    "     'learning': 6\n",
    "     'machine': 7\n",
    "     'make': 8\n",
    "     ```\n",
    "\n",
    "3. **Matrix Construction**\n",
    "   - For each document, count the occurrences of each token in the vocabulary and create a vector for each document.\n",
    "\n",
    "### Detailed Example\n",
    "\n",
    "**Document 1: \"Machine learning is fun.\"**\n",
    "- Tokenized: `[\"machine\", \"learning\", \"is\", \"fun\"]`\n",
    "- Vocabulary indices:\n",
    "  - \"be\": 0 occurrences\n",
    "  - \"can\": 0 occurrences\n",
    "  - \"challenging\": 0 occurrences\n",
    "  - \"challenges\": 0 occurrences\n",
    "  - \"fun\": 1 occurrence\n",
    "  - \"is\": 1 occurrence\n",
    "  - \"learning\": 1 occurrence\n",
    "  - \"machine\": 1 occurrence\n",
    "  - \"make\": 0 occurrences\n",
    "- Vector: `[0, 0, 0, 0, 1, 1, 1, 1, 0]`\n",
    "\n",
    "**Document 2: \"Learning machine learning can be challenging.\"**\n",
    "- Tokenized: `[\"learning\", \"machine\", \"learning\", \"can\", \"be\", \"challenging\"]`\n",
    "- Vocabulary indices:\n",
    "  - \"be\": 1 occurrence\n",
    "  - \"can\": 1 occurrence\n",
    "  - \"challenging\": 1 occurrence\n",
    "  - \"challenges\": 0 occurrences\n",
    "  - \"fun\": 0 occurrences\n",
    "  - \"is\": 0 occurrences\n",
    "  - \"learning\": 2 occurrences\n",
    "  - \"machine\": 1 occurrence\n",
    "  - \"make\": 0 occurrences\n",
    "- Vector: `[1, 1, 1, 0, 0, 0, 2, 1, 0]`\n",
    "\n",
    "**Document 3: \"Challenges make learning fun.\"**\n",
    "- Tokenized: `[\"challenges\", \"make\", \"learning\", \"fun\"]`\n",
    "- Vocabulary indices:\n",
    "  - \"be\": 0 occurrences\n",
    "  - \"can\": 0 occurrences\n",
    "  - \"challenging\": 0 occurrences\n",
    "  - \"challenges\": 1 occurrence\n",
    "  - \"fun\": 1 occurrence\n",
    "  - \"is\": 0 occurrences\n",
    "  - \"learning\": 1 occurrence\n",
    "  - \"machine\": 0 occurrences\n",
    "  - \"make\": 1 occurrence\n",
    "- Vector: `[0, 0, 0, 1, 1, 0, 1, 0, 1]`\n",
    "\n",
    "### Final Document-Term Matrix\n",
    "Combining the vectors for all documents, we get the final matrix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e6242",
   "metadata": {},
   "source": [
    "- [[0 0 0 0 1 1 1 1 0]  # \"Machine learning is fun.\"\n",
    "-  [1 1 1 0 0 0 2 1 0]  # \"Learning machine learning can be challenging.\"\n",
    "-  [0 0 0 1 1 0 1 0 1]] # \"Challenges make learning fun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26101c",
   "metadata": {},
   "source": [
    "\n",
    "This matrix represents the counts of each token from the vocabulary in each document. Each row corresponds to a document, and each column corresponds to a token from the vocabulary. The value at a specific row and column indicates the count of that token in the corresponding document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fee8e",
   "metadata": {},
   "source": [
    "### Benefits of Using CountVectorizer\n",
    "\n",
    "- **Simplicity**: It's easy to use and understand, making it a good starting point for text feature extraction.\n",
    "- **Effectiveness**: Works well for many text classification tasks where the frequency of words matters.\n",
    "- **Compatibility**: Integrated with `scikit-learn`, allowing for seamless use with various machine learning models and pipelines.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Sparsity**: The resulting document-term matrix can be very large and sparse, especially for large vocabularies.\n",
    "- **Loss of Context**: CountVectorizer does not consider the order of words or context; it only counts occurrences.\n",
    "- **Feature Scaling**: It does not normalize or scale the token counts, which might be necessary for some machine learning algorithms.\n",
    "\n",
    "To address some of these limitations, other techniques like TF-IDF (Term Frequency-Inverse Document Frequency) vectorization or word embeddings (e.g., Word2Vec, GloVe) can be used for more sophisticated text representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f42c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
