{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9d7ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9519d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df9c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "\n",
    "nltk.download('punkt')   #pre-trained ML model for tokenization \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ad407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text=\"Hello, World! This is an example of Text History Preprocessing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5dea3",
   "metadata": {},
   "source": [
    "# 1. Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed8ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text : hello world this is an example of text history preprocessing\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation \n",
    "\n",
    "text = text.translate(str.maketrans('','',string.punctuation))\n",
    "print(\"Preprocessed Text :\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a13e0",
   "metadata": {},
   "source": [
    "# 2. Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86ce652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['hello', 'world', 'this', 'is', 'an', 'example', 'of', 'text', 'history', 'preprocessing']\n",
      "Sentence Tokens: ['hello world this is an example of text history preprocessing']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\",word_tokens)\n",
    "\n",
    "# Senetence Tokenization\n",
    "\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e591d",
   "metadata": {},
   "source": [
    "# 3. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f9a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Words Removal: ['hello', 'world', 'example', 'text', 'history', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens=[word for word in word_tokens if word not in stop_words]\n",
    "print(\"Tokens after Stop Words Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b32f07",
   "metadata": {},
   "source": [
    "# 4. Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d0f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['hello', 'world', 'exampl', 'text', 'histori', 'preprocess']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\",stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb064b2c",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82812d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['hello', 'world', 'example', 'text', 'history', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\",lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cfb485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-Speech Tags: [('hello', 'JJ'), ('world', 'NN'), ('this', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('of', 'IN'), ('text', 'JJ'), ('history', 'NN'), ('preprocessing', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 6. Part-of_speech Tagging \n",
    "\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"Part-of-Speech Tags:\",pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e95a5",
   "metadata": {},
   "source": [
    "#### Corpus\n",
    "In NLTK, a corpus is a large and structured set of texts. It is used to provide linguistic data for language processing tasks. NLTK includes several corpora that are commonly used for training and evaluating NLP models. These corpora can include raw text, tagged text, and annotated text, among other forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49837456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
